struct ParellelReduceHandler {
    is_gpu: bool,
    dev_id: i32,
    acc: Accelerator,
}

fn @make_cpu_parallel_reduce_handler()                              = ParellelReduceHandler { is_gpu = false, dev_id = 0,      acc = undef[Accelerator]() };
fn @make_gpu_parallel_reduce_handler(dev_id: i32, acc: Accelerator) = ParellelReduceHandler { is_gpu = true,  dev_id = dev_id, acc = acc };

/// Used to abstract the T in a more generic way
fn @reduce[T](handler: ParellelReduceHandler, n: i32, elem: fn (i32) -> T, op: fn (T, T) -> T) -> T {
    if handler.is_gpu {
        gpu_handle_device_reduce[T](handler.dev_id, handler.acc, n, elem, op)
    } else {
        cpu_reduce[T](n, elem, op)
    }
}

/// GPU specific
fn @gpu_handle_device_reduce[T]( dev_id: i32
                               , acc: Accelerator
                               , n: i32
                               , elem: fn (i32) -> T
                               , op: fn (T, T) -> T) -> T {
    let block_size = if ?n && n < 10000 {
        128
    } else if ?n && n < 1000000 {
        256
    } else {
        512
    };

    // TODO: This forces us to only use one reduce call at a single time (which is fair tbh)
    let mut tmp_ptr : &[u8];
    ignis_request_buffer(dev_id, "__dev_tmp_reduce", &mut tmp_ptr, sizeof[T]() as i32, 0);

    let kernel_ptr = tmp_ptr as &mut addrspace(1)[T];
    gpu_reduce[T](acc, n, block_size,
        elem, op,
        @|v| kernel_ptr(0) = v
    );
    acc.sync();

    let mut value: T;
    runtime_copy(dev_id, tmp_ptr as &[i8], 0, 0 /* Host */, &mut value as &mut [i8], 0, sizeof[T]());
    
    value
}