struct ParellelReduceHandler {
    is_gpu: bool,
    dev_id: i32,
    acc: Accelerator,
}

fn @make_cpu_parallel_reduce_handler()                              = ParellelReduceHandler { is_gpu = false, dev_id = 0,      acc = undef[Accelerator]() };
fn @make_gpu_parallel_reduce_handler(dev_id: i32, acc: Accelerator) = ParellelReduceHandler { is_gpu = true,  dev_id = dev_id, acc = acc };

/// Used to abstract the T in a more generic way
fn @reduce[T](handler: ParellelReduceHandler, n: i32, elem: fn (i32) -> T, op: fn (T, T) -> T) -> T {
    if handler.is_gpu {
        gpu_handle_device_reduce[T](handler.dev_id, handler.acc, n, elem, op)
    } else {
        cpu_reduce[T](n, elem, op)
    }
}

/// GPU specific
fn @gpu_handle_device_reduce[T]( dev_id: i32
                               , acc: Accelerator
                               , n: i32
                               , elem: fn (i32) -> T
                               , op: fn (T, T) -> T) -> T {
    fn @reducer(block_size: i32) -> T {
        // TODO: This forces us to only use one reduce call at a single time (which is fair tbh)
        let mut tmp_ptr : &[u8];
        ignis_request_buffer(dev_id, "__dev_tmp_reduce", &mut tmp_ptr, sizeof[T]() as i32, 0);

        let kernel_ptr = tmp_ptr as &mut addrspace(1)[T];
        gpu_reduce[T](acc, n, block_size,
            elem, op,
            @|v| kernel_ptr(0) = v
        );
        acc.sync();

        let mut value: T;
        runtime_copy(dev_id, tmp_ptr as &[i8], 0, 0 /* Host */, &mut value as &mut [i8], 0, sizeof[T]());
        
        value
    }

    if n < 10000 {
        reducer(128)
    } else if n < 1000000 {
        reducer(256)      
    } else {
        reducer(512)
    }
}